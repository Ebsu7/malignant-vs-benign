{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimages_path=[]\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        images_path.append(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-04T21:16:24.570464Z","iopub.execute_input":"2023-09-04T21:16:24.570759Z","iopub.status.idle":"2023-09-04T21:16:28.056779Z","shell.execute_reply.started":"2023-09-04T21:16:24.570733Z","shell.execute_reply":"2023-09-04T21:16:28.055647Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:16:28.062704Z","iopub.execute_input":"2023-09-04T21:16:28.065807Z","iopub.status.idle":"2023-09-04T21:16:43.747465Z","shell.execute_reply.started":"2023-09-04T21:16:28.065751Z","shell.execute_reply":"2023-09-04T21:16:43.746407Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'2.12.0'"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.models import \n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:16:43.749448Z","iopub.execute_input":"2023-09-04T21:16:43.750937Z","iopub.status.idle":"2023-09-04T21:16:43.757291Z","shell.execute_reply.started":"2023-09-04T21:16:43.750900Z","shell.execute_reply":"2023-09-04T21:16:43.756217Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create an ImageDataGenerator object for data augmentation and preprocessing\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,              # Rescale pixel values to the range [0, 1]\n    shear_range=0.2,             # Apply random shear transformations\n    zoom_range=0.2,              # Apply random zoom transformations\n    horizontal_flip=True        # Flip images horizontally\n)\n\n# Generate a flow of training data from the directory\ntraining_set = train_datagen.flow_from_directory(\n    '../input/skin-cancer-malignant-vs-benign/train',   # Directory containing the training images\n    target_size=(64, 64),                              # Resize images to (64, 64) pixels\n    batch_size=10,                                     # Number of images per batch\n    class_mode='binary'                                # Type of classification task (binary in this case)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T19:43:04.383999Z","iopub.execute_input":"2023-09-04T19:43:04.384359Z","iopub.status.idle":"2023-09-04T19:43:05.490563Z","shell.execute_reply.started":"2023-09-04T19:43:04.384330Z","shell.execute_reply":"2023-09-04T19:43:05.489622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an ImageDataGenerator object for test data preprocessing\ntest_datagen = ImageDataGenerator(\n    rescale=1./255  # Rescale pixel values to the range [0, 1]\n)\n\n# Generate a flow of test data from the directory\ntest_set = test_datagen.flow_from_directory(\n    '../input/skin-cancer-malignant-vs-benign/test',  # Directory containing the test images\n    target_size=(64, 64),                             # Resize images to (64, 64) pixels\n    batch_size=10,                                    # Number of images per batch\n    class_mode='binary'                               # Type of classification task (binary in this case)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T19:43:08.618980Z","iopub.execute_input":"2023-09-04T19:43:08.619342Z","iopub.status.idle":"2023-09-04T19:43:08.668515Z","shell.execute_reply.started":"2023-09-04T19:43:08.619312Z","shell.execute_reply":"2023-09-04T19:43:08.667370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:16:43.758905Z","iopub.execute_input":"2023-09-04T21:16:43.759265Z","iopub.status.idle":"2023-09-04T21:16:44.264541Z","shell.execute_reply.started":"2023-09-04T21:16:43.759231Z","shell.execute_reply":"2023-09-04T21:16:44.263438Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Define image dimensions\nimg_width, img_height = 128, 128\n\n# Load and preprocess images\ndef load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        img = load_img(os.path.join(folder, filename), target_size=(img_width, img_height))\n        img = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n        images.append(img)\n    return np.array(images)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:16:44.267372Z","iopub.execute_input":"2023-09-04T21:16:44.267800Z","iopub.status.idle":"2023-09-04T21:16:44.275940Z","shell.execute_reply.started":"2023-09-04T21:16:44.267760Z","shell.execute_reply":"2023-09-04T21:16:44.274650Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load benign and malignant train images\nbenign_folder_train = '/kaggle/input/skin-cancer-malignant-vs-benign/train/benign'\nmalignant_folder_train ='/kaggle/input/skin-cancer-malignant-vs-benign/train/malignant'\n\nbenign_images_train = load_images_from_folder(benign_folder_train)\nmalignant_images_train = load_images_from_folder(malignant_folder_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:16:48.135875Z","iopub.execute_input":"2023-09-04T21:16:48.137112Z","iopub.status.idle":"2023-09-04T21:17:09.409380Z","shell.execute_reply.started":"2023-09-04T21:16:48.137069Z","shell.execute_reply":"2023-09-04T21:17:09.408348Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load benign and malignant train images\nbenign_folder_test = '/kaggle/input/skin-cancer-malignant-vs-benign/test/benign'\nmalignant_folder_test ='/kaggle/input/skin-cancer-malignant-vs-benign/test/malignant'\n\nbenign_images_test = load_images_from_folder(benign_folder_test)\nmalignant_images_test = load_images_from_folder(malignant_folder_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:17:16.410534Z","iopub.execute_input":"2023-09-04T21:17:16.410909Z","iopub.status.idle":"2023-09-04T21:17:18.189964Z","shell.execute_reply.started":"2023-09-04T21:17:16.410882Z","shell.execute_reply":"2023-09-04T21:17:18.188982Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create labels for the images\nbenign_labels_train = np.zeros(len(benign_images_train))\nmalignant_labels_train = np.ones(len(malignant_images_train))\n\nbenign_labels_test = np.zeros(len(benign_images_test))\nmalignant_labels_test = np.ones(len(malignant_images_test))\n\n\n# Concatenate images and labels\n#train_images, test_images, train_labels, test_labels\ntrain_images = np.concatenate((benign_images_train, malignant_images_train), axis=0)\ntrain_labels = np.concatenate((benign_labels_train, malignant_labels_train), axis=0)\n\n\ntest_images = np.concatenate((benign_images_test, malignant_images_test), axis=0)\ntest_labels = np.concatenate((benign_labels_test, malignant_labels_test), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:17:18.191362Z","iopub.execute_input":"2023-09-04T21:17:18.191746Z","iopub.status.idle":"2023-09-04T21:17:18.427661Z","shell.execute_reply.started":"2023-09-04T21:17:18.191712Z","shell.execute_reply":"2023-09-04T21:17:18.426678Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Apply autoencoder to reconstruct images, then add dense layers after the last Idecoder layer for classification.","metadata":{}},{"cell_type":"code","source":"# Autoencoder model\ninput_img = Input(shape=(img_width, img_height, 3))\nencoded = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nencoded = MaxPooling2D((2, 2), padding='same')(encoded)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(encoded)\ndecoded = UpSampling2D((2, 2))(decoded)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Classification model\nclassification_input = autoencoder.input\nclassification_output = autoencoder.layers[-2].output\nclassification_output = Flatten()(classification_output)\nclassification_output = Dense(128, activation='relu')(classification_output)\nclassification_output = Dense(1, activation='sigmoid')(classification_output)\n\nclassification_model = Model(classification_input, classification_output)\nclassification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nclassification_model.summary()\n\n# Train the autoencoder\nautoencoder.fit(train_images, train_images, epochs=10, batch_size=32, validation_data=(test_images, test_images))\n\n# Train the classification model\nclassification_model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:17:18.428991Z","iopub.execute_input":"2023-09-04T21:17:18.429420Z","iopub.status.idle":"2023-09-04T21:18:31.036573Z","shell.execute_reply.started":"2023-09-04T21:17:18.429387Z","shell.execute_reply":"2023-09-04T21:18:31.035376Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 128, 128, 16)      448       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 64, 64, 16)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 64, 64, 3)         435       \n                                                                 \n flatten (Flatten)           (None, 12288)             0         \n                                                                 \n dense (Dense)               (None, 128)               1572992   \n                                                                 \n dense_1 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,574,004\nTrainable params: 1,574,004\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/10\n83/83 [==============================] - 16s 25ms/step - loss: 0.6288 - val_loss: 0.6062\nEpoch 2/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5997 - val_loss: 0.5926\nEpoch 3/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5909 - val_loss: 0.5867\nEpoch 4/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5874 - val_loss: 0.5847\nEpoch 5/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5863 - val_loss: 0.5841\nEpoch 6/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5860 - val_loss: 0.5838\nEpoch 7/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5858 - val_loss: 0.5837\nEpoch 8/10\n83/83 [==============================] - 1s 17ms/step - loss: 0.5857 - val_loss: 0.5836\nEpoch 9/10\n83/83 [==============================] - 1s 17ms/step - loss: 0.5856 - val_loss: 0.5836\nEpoch 10/10\n83/83 [==============================] - 1s 14ms/step - loss: 0.5856 - val_loss: 0.5836\nEpoch 1/10\n83/83 [==============================] - 4s 17ms/step - loss: 1.0849 - accuracy: 0.6397 - val_loss: 0.5783 - val_accuracy: 0.7333\nEpoch 2/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.5346 - accuracy: 0.7327 - val_loss: 0.5572 - val_accuracy: 0.7742\nEpoch 3/10\n83/83 [==============================] - 1s 11ms/step - loss: 0.4839 - accuracy: 0.7573 - val_loss: 0.4646 - val_accuracy: 0.7833\nEpoch 4/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.4682 - accuracy: 0.7687 - val_loss: 0.4358 - val_accuracy: 0.7621\nEpoch 5/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.4581 - accuracy: 0.7736 - val_loss: 0.5077 - val_accuracy: 0.7773\nEpoch 6/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.4400 - accuracy: 0.7801 - val_loss: 0.4153 - val_accuracy: 0.7758\nEpoch 7/10\n83/83 [==============================] - 1s 11ms/step - loss: 0.4377 - accuracy: 0.7873 - val_loss: 0.4339 - val_accuracy: 0.8000\nEpoch 8/10\n83/83 [==============================] - 1s 11ms/step - loss: 0.4120 - accuracy: 0.8024 - val_loss: 0.4645 - val_accuracy: 0.7955\nEpoch 9/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.4113 - accuracy: 0.7975 - val_loss: 0.4723 - val_accuracy: 0.7788\nEpoch 10/10\n83/83 [==============================] - 1s 10ms/step - loss: 0.4174 - accuracy: 0.7899 - val_loss: 0.3970 - val_accuracy: 0.7833\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7945fc266410>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom tensorflow.keras.preprocessing.image import img_to_array\n\n# Load and preprocess images\ndef load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        if img is not None:\n            img = cv2.resize(img, (128, 128))  # Resize image to desired dimensions\n            img = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n            img = img.astype('float32') / 255.0\n            images.append(img)\n    return np.array(images)\n\n\n# Load benign and malignant train images\nbenign_folder_train = '/kaggle/input/skin-cancer-malignant-vs-benign/train/benign'\nmalignant_folder_train ='/kaggle/input/skin-cancer-malignant-vs-benign/train/malignant'\n\nbenign_images_train = load_images_from_folder(benign_folder_train)\nmalignant_images_train = load_images_from_folder(malignant_folder_train)\n\n\n# Load benign and malignant train images\nbenign_folder_test = '/kaggle/input/skin-cancer-malignant-vs-benign/test/benign'\nmalignant_folder_test ='/kaggle/input/skin-cancer-malignant-vs-benign/test/malignant'\n\nbenign_images_test = load_images_from_folder(benign_folder_test)\nmalignant_images_test = load_images_from_folder(malignant_folder_test)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:39:09.320546Z","iopub.execute_input":"2023-09-04T21:39:09.320957Z","iopub.status.idle":"2023-09-04T21:39:24.329396Z","shell.execute_reply.started":"2023-09-04T21:39:09.320926Z","shell.execute_reply":"2023-09-04T21:39:24.328227Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Apply autoencoder to reconstruct images, then add dense layers that takes the output of the encoder (bottleneck as input) for classification.","metadata":{}},{"cell_type":"code","source":"#Apply autoencoder to reconstruct images, then add dense layers that takes the output of the encoder (bottleneck ) for classification\n# Concatenate images and create labels\nx = np.concatenate((benign_images_train, malignant_images_train), axis=0)\ny = np.concatenate((np.zeros(len(benign_images_train)), np.ones(len(malignant_images_train))), axis=0)\ny = np.expand_dims(y, axis=-1)  # Reshape labels to match the classification output shape\n\n# Autoencoder model\ninput_img = Input(shape=(128, 128, 3))\nencoded = Dense(128, activation='relu')(input_img)\ndecoded = Dense(3, activation='sigmoid')(encoded)\n\nautoencoder = Model(input_img, decoded)\n\n# Encoder model (bottleneck)\nencoder = Model(input_img, encoded)\n\n# Dense layers for classification\nflatten = Flatten()(encoded)\nclassification_output = Dense(1, activation='sigmoid')(flatten)\n\nclassification_model = Model(input_img, classification_output)\n\n# Compile and train the classification model\nclassification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nclassification_model.fit(x, y, batch_size=32, epochs=10, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:40:50.570759Z","iopub.execute_input":"2023-09-04T21:40:50.571142Z","iopub.status.idle":"2023-09-04T21:41:33.786199Z","shell.execute_reply.started":"2023-09-04T21:40:50.571110Z","shell.execute_reply":"2023-09-04T21:41:33.785171Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/10\n66/66 [==============================] - 4s 44ms/step - loss: 0.6668 - accuracy: 0.6411 - val_loss: 0.9307 - val_accuracy: 0.0000e+00\nEpoch 2/10\n66/66 [==============================] - 2s 38ms/step - loss: 0.6302 - accuracy: 0.6828 - val_loss: 1.0212 - val_accuracy: 0.0000e+00\nEpoch 3/10\n66/66 [==============================] - 2s 38ms/step - loss: 0.6278 - accuracy: 0.6828 - val_loss: 1.2002 - val_accuracy: 0.0000e+00\nEpoch 4/10\n66/66 [==============================] - 3s 38ms/step - loss: 0.6242 - accuracy: 0.6828 - val_loss: 1.0419 - val_accuracy: 0.0000e+00\nEpoch 5/10\n66/66 [==============================] - 3s 38ms/step - loss: 0.6250 - accuracy: 0.6828 - val_loss: 0.9573 - val_accuracy: 0.0000e+00\nEpoch 6/10\n66/66 [==============================] - 2s 38ms/step - loss: 0.6241 - accuracy: 0.6828 - val_loss: 1.1015 - val_accuracy: 0.0000e+00\nEpoch 7/10\n66/66 [==============================] - 3s 40ms/step - loss: 0.6238 - accuracy: 0.6828 - val_loss: 1.2081 - val_accuracy: 0.0000e+00\nEpoch 8/10\n66/66 [==============================] - 3s 38ms/step - loss: 0.6346 - accuracy: 0.6828 - val_loss: 1.0184 - val_accuracy: 0.0000e+00\nEpoch 9/10\n66/66 [==============================] - 2s 38ms/step - loss: 0.6270 - accuracy: 0.6828 - val_loss: 1.0427 - val_accuracy: 0.0000e+00\nEpoch 10/10\n66/66 [==============================] - 2s 38ms/step - loss: 0.6252 - accuracy: 0.6828 - val_loss: 1.0810 - val_accuracy: 0.0000e+00\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7946443a5660>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Reshape\nfrom keras.models import Model\n\n# Load and preprocess images\ndef load_images_from_folder(folder):\n    images = []\n    for filename in os.listdir(folder):\n        img = cv2.imread(os.path.join(folder, filename))\n        if img is not None:\n            img = cv2.resize(img, (128, 128))  # Resize image to desired dimensions\n            img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n            images.append(img)\n    return np.array(images)\n\n# Load benign and malignant train images\nbenign_folder_train = '/kaggle/input/skin-cancer-malignant-vs-benign/train/benign'\nmalignant_folder_train ='/kaggle/input/skin-cancer-malignant-vs-benign/train/malignant'\n\nbenign_images_train = load_images_from_folder(benign_folder_train)\nmalignant_images_train = load_images_from_folder(malignant_folder_train)\n\n\n# Load benign and malignant train images\nbenign_folder_test = '/kaggle/input/skin-cancer-malignant-vs-benign/test/benign'\nmalignant_folder_test ='/kaggle/input/skin-cancer-malignant-vs-benign/test/malignant'\n\nbenign_images_test = load_images_from_folder(benign_folder_test)\nmalignant_images_test = load_images_from_folder(malignant_folder_test)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:48:36.493641Z","iopub.execute_input":"2023-09-04T21:48:36.494029Z","iopub.status.idle":"2023-09-04T21:49:01.638886Z","shell.execute_reply.started":"2023-09-04T21:48:36.493998Z","shell.execute_reply":"2023-09-04T21:49:01.637668Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"After applying autoencoder, reshape the output vector of the last decoder layer and then add CNN layers and finally add Dense layers for classification","metadata":{}},{"cell_type":"code","source":"# Autoencoder model\ninput_img = Input(shape=(img_width, img_height, 3))\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nencoded = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n\n# Decoder layers\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\n# Reshape the output vector of the last decoder layer\nflatten_layer = Flatten()(encoded)\nreshaped_output = Reshape((8, 8, 8))(flatten_layer)\n\n# CNN layers\ncnn = Conv2D(16, (3, 3), activation='relu', padding='same')(reshaped_output)\ncnn = MaxPooling2D((2, 2), padding='same')(cnn)\ncnn = Conv2D(32, (3, 3), activation='relu', padding='same')(cnn)\ncnn = MaxPooling2D((2, 2), padding='same')(cnn)\n\n# Flatten and dense layers for classification\nflatten = Flatten()(cnn)\ndense = Dense(128, activation='relu')(flatten)\noutput = Dense(1, activation='sigmoid')(dense)\n\nclassification_model = Model(input_img, output)\nclassification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nclassification_model.summary()\n\n# Train the classification model\nclassification_model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:51:31.777804Z","iopub.execute_input":"2023-09-04T21:51:31.778246Z","iopub.status.idle":"2023-09-04T21:51:31.998914Z","shell.execute_reply.started":"2023-09-04T21:51:31.778211Z","shell.execute_reply":"2023-09-04T21:51:31.997528Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Reshape the output vector of the last decoder layer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m flatten_layer \u001b[38;5;241m=\u001b[39m Flatten()(encoded)\n\u001b[0;32m---> 14\u001b[0m reshaped_output \u001b[38;5;241m=\u001b[39m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# CNN layers\u001b[39;00m\n\u001b[1;32m     17\u001b[0m cnn \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m16\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)(reshaped_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_7\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [32768], output_shape = [8, 8, 8]\n\nCall arguments received by layer \"reshape_7\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 32768), dtype=float32)"],"ename":"ValueError","evalue":"Exception encountered when calling layer \"reshape_7\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [32768], output_shape = [8, 8, 8]\n\nCall arguments received by layer \"reshape_7\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 32768), dtype=float32)","output_type":"error"}]},{"cell_type":"markdown","source":"After applying autoencoder, reshape the output vector of the last decoder layer and then add Resnetse (transfer learning), CNN layers and finally add Dense layers for classification.","metadata":{}},{"cell_type":"code","source":"# Autoencoder model\ninput_img = Input(shape=(128, 128, 3))\nencoded = Dense(128, activation='relu')(input_img)\ndecoded = Dense(16 * 16 * 3, activation='relu')(encoded)\nreshaped_output = Reshape((16, 16, 3))(decoded)  # Reshape the output to (16, 16, 3)\n\n# Transfer learning with ResNet50\nresnet_model = ResNet50(weights='imagenet', include_top=False, input_tensor=reshaped_output)\nresnet_output = resnet_model.output\n\n# CNN layers\nconv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(resnet_output)\npool1 = MaxPooling2D((2, 2))(conv1)\nconv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\npool2 = MaxPooling2D((2, 2))(conv2)\nflatten = Flatten()(pool2)\n\n# Dense layers for classification\nclassification_output = Dense(1, activation='sigmoid')(flatten)\n\nclassification_model = Model(input_img, classification_output)\n\n# Compile and train the classification model\nclassification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nclassification_model.fit(x, y, batch_size=32, epochs=10, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:52:31.013491Z","iopub.execute_input":"2023-09-04T21:52:31.014640Z","iopub.status.idle":"2023-09-04T21:52:31.147997Z","shell.execute_reply.started":"2023-09-04T21:52:31.014588Z","shell.execute_reply":"2023-09-04T21:52:31.146501Z"},"trusted":true},"execution_count":36,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m encoded \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(input_img)\n\u001b[1;32m      4\u001b[0m decoded \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(encoded)\n\u001b[0;32m----> 5\u001b[0m reshaped_output \u001b[38;5;241m=\u001b[39m \u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reshape the output to (16, 16, 3)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Transfer learning with ResNet50\u001b[39;00m\n\u001b[1;32m      8\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m ResNet50(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_tensor\u001b[38;5;241m=\u001b[39mreshaped_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_8\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [128, 128, 768], output_shape = [16, 16, 3]\n\nCall arguments received by layer \"reshape_8\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 128, 128, 768), dtype=float32)"],"ename":"ValueError","evalue":"Exception encountered when calling layer \"reshape_8\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [128, 128, 768], output_shape = [16, 16, 3]\n\nCall arguments received by layer \"reshape_8\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 128, 128, 768), dtype=float32)","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Apply only CNN","metadata":{}},{"cell_type":"code","source":"# Apply only CNN\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Check TensorFlow version\ntf.__version__\n\n# Data augmentation and preprocessing for the training set\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\n# Load and preprocess the training set\ntraining_set = train_datagen.flow_from_directory(\n    '../input/skin-cancer-malignant-vs-benign/train',\n    target_size=(64, 64),\n    batch_size=10,\n    class_mode='binary'\n)\n\n# Data preprocessing for the test set\ntest_datagen = ImageDataGenerator(\n    rescale=1./255\n)\n\n# Load and preprocess the test set\ntest_set = test_datagen.flow_from_directory(\n    '../input/skin-cancer-malignant-vs-benign/test',\n    target_size=(64, 64),\n    batch_size=10,\n    class_mode='binary'\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-05T22:41:37.801470Z","iopub.execute_input":"2023-09-05T22:41:37.801829Z","iopub.status.idle":"2023-09-05T22:41:39.664557Z","shell.execute_reply.started":"2023-09-05T22:41:37.801800Z","shell.execute_reply":"2023-09-05T22:41:39.663605Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 2637 images belonging to 2 classes.\nFound 660 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Create a Sequential model\ncnn = tf.keras.models.Sequential()\n\n# Add a convolutional layer with 32 filters, a kernel size of 3x3, and ReLU activation\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n\n# Add a max pooling layer with a pool size of 2x2 and stride of 2\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n\n# Add another convolutional layer with 32 filters, a kernel size of 3x3, and ReLU activation\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n\n# Add another max pooling layer with a pool size of 2x2 and stride of 2\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n\n# Flatten the output from the previous layer\ncnn.add(tf.keras.layers.Flatten())\n\n# Add a dense layer with 128 units and ReLU activation\ncnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n\n# Add an output layer with 1 unit (for binary classification) and sigmoid activation\ncnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\n# Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\ncnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model using the training set and validate on the test set for 50 epochs\ncnn.fit(x=training_set, validation_data=test_set, epochs=50)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-04T21:58:47.452408Z","iopub.execute_input":"2023-09-04T21:58:47.452805Z","iopub.status.idle":"2023-09-04T22:13:14.131758Z","shell.execute_reply.started":"2023-09-04T21:58:47.452772Z","shell.execute_reply":"2023-09-04T22:13:14.130639Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 1/50\n264/264 [==============================] - 16s 53ms/step - loss: 0.6332 - accuracy: 0.6231 - val_loss: 0.5029 - val_accuracy: 0.7682\nEpoch 2/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.4924 - accuracy: 0.7637 - val_loss: 0.4126 - val_accuracy: 0.7955\nEpoch 3/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.4434 - accuracy: 0.7744 - val_loss: 0.4074 - val_accuracy: 0.8015\nEpoch 4/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.4046 - accuracy: 0.7983 - val_loss: 0.4129 - val_accuracy: 0.7939\nEpoch 5/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.3974 - accuracy: 0.8074 - val_loss: 0.3810 - val_accuracy: 0.8015\nEpoch 6/50\n264/264 [==============================] - 14s 52ms/step - loss: 0.4083 - accuracy: 0.7956 - val_loss: 0.3839 - val_accuracy: 0.8030\nEpoch 7/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.4031 - accuracy: 0.8009 - val_loss: 0.3711 - val_accuracy: 0.8197\nEpoch 8/50\n264/264 [==============================] - 14s 51ms/step - loss: 0.3910 - accuracy: 0.8074 - val_loss: 0.3809 - val_accuracy: 0.8106\nEpoch 9/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.3777 - accuracy: 0.8191 - val_loss: 0.3540 - val_accuracy: 0.8318\nEpoch 10/50\n264/264 [==============================] - 14s 54ms/step - loss: 0.3784 - accuracy: 0.8074 - val_loss: 0.3467 - val_accuracy: 0.8409\nEpoch 11/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.3716 - accuracy: 0.8142 - val_loss: 0.3405 - val_accuracy: 0.8242\nEpoch 12/50\n264/264 [==============================] - 14s 53ms/step - loss: 0.3578 - accuracy: 0.8309 - val_loss: 0.3478 - val_accuracy: 0.8167\nEpoch 13/50\n264/264 [==============================] - 14s 52ms/step - loss: 0.3526 - accuracy: 0.8294 - val_loss: 0.3702 - val_accuracy: 0.8227\nEpoch 14/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.3437 - accuracy: 0.8297 - val_loss: 0.3449 - val_accuracy: 0.8273\nEpoch 15/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.3432 - accuracy: 0.8354 - val_loss: 0.3297 - val_accuracy: 0.8348\nEpoch 16/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.3331 - accuracy: 0.8400 - val_loss: 0.3558 - val_accuracy: 0.8273\nEpoch 17/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.3189 - accuracy: 0.8430 - val_loss: 0.3698 - val_accuracy: 0.8106\nEpoch 18/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.3283 - accuracy: 0.8377 - val_loss: 0.3354 - val_accuracy: 0.8227\nEpoch 19/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.3039 - accuracy: 0.8532 - val_loss: 0.3687 - val_accuracy: 0.8288\nEpoch 20/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.2995 - accuracy: 0.8612 - val_loss: 0.3496 - val_accuracy: 0.8333\nEpoch 21/50\n264/264 [==============================] - 14s 51ms/step - loss: 0.2963 - accuracy: 0.8642 - val_loss: 0.3404 - val_accuracy: 0.8318\nEpoch 22/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2855 - accuracy: 0.8695 - val_loss: 0.3581 - val_accuracy: 0.8470\nEpoch 23/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.3013 - accuracy: 0.8623 - val_loss: 0.3645 - val_accuracy: 0.8288\nEpoch 24/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2922 - accuracy: 0.8665 - val_loss: 0.3442 - val_accuracy: 0.8394\nEpoch 25/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.2871 - accuracy: 0.8684 - val_loss: 0.3476 - val_accuracy: 0.8424\nEpoch 26/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.2823 - accuracy: 0.8756 - val_loss: 0.3561 - val_accuracy: 0.8348\nEpoch 27/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2774 - accuracy: 0.8760 - val_loss: 0.3561 - val_accuracy: 0.8303\nEpoch 28/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.2661 - accuracy: 0.8779 - val_loss: 0.4013 - val_accuracy: 0.8333\nEpoch 29/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2772 - accuracy: 0.8688 - val_loss: 0.4210 - val_accuracy: 0.8258\nEpoch 30/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.2609 - accuracy: 0.8745 - val_loss: 0.3581 - val_accuracy: 0.8394\nEpoch 31/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2594 - accuracy: 0.8817 - val_loss: 0.3584 - val_accuracy: 0.8364\nEpoch 32/50\n264/264 [==============================] - 14s 51ms/step - loss: 0.2398 - accuracy: 0.8904 - val_loss: 0.3755 - val_accuracy: 0.8379\nEpoch 33/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2373 - accuracy: 0.8938 - val_loss: 0.3796 - val_accuracy: 0.8258\nEpoch 34/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.2385 - accuracy: 0.8923 - val_loss: 0.3606 - val_accuracy: 0.8455\nEpoch 35/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2511 - accuracy: 0.8927 - val_loss: 0.3665 - val_accuracy: 0.8364\nEpoch 36/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.2321 - accuracy: 0.8904 - val_loss: 0.3970 - val_accuracy: 0.8485\nEpoch 37/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.2270 - accuracy: 0.9056 - val_loss: 0.3755 - val_accuracy: 0.8530\nEpoch 38/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2270 - accuracy: 0.9003 - val_loss: 0.4192 - val_accuracy: 0.8470\nEpoch 39/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.2063 - accuracy: 0.9101 - val_loss: 0.3645 - val_accuracy: 0.8485\nEpoch 40/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.2161 - accuracy: 0.8976 - val_loss: 0.3642 - val_accuracy: 0.8409\nEpoch 41/50\n264/264 [==============================] - 14s 51ms/step - loss: 0.2108 - accuracy: 0.9052 - val_loss: 0.3921 - val_accuracy: 0.8303\nEpoch 42/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 0.4012 - val_accuracy: 0.8348\nEpoch 43/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.1949 - accuracy: 0.9120 - val_loss: 0.3751 - val_accuracy: 0.8485\nEpoch 44/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.2073 - accuracy: 0.9082 - val_loss: 0.4149 - val_accuracy: 0.8364\nEpoch 45/50\n264/264 [==============================] - 13s 49ms/step - loss: 0.1995 - accuracy: 0.9128 - val_loss: 0.4565 - val_accuracy: 0.8348\nEpoch 46/50\n264/264 [==============================] - 13s 50ms/step - loss: 0.2117 - accuracy: 0.9120 - val_loss: 0.4062 - val_accuracy: 0.8455\nEpoch 47/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.1764 - accuracy: 0.9261 - val_loss: 0.4748 - val_accuracy: 0.8197\nEpoch 48/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.1846 - accuracy: 0.9207 - val_loss: 0.4073 - val_accuracy: 0.8379\nEpoch 49/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.1709 - accuracy: 0.9287 - val_loss: 0.5015 - val_accuracy: 0.8242\nEpoch 50/50\n264/264 [==============================] - 13s 51ms/step - loss: 0.1925 - accuracy: 0.9200 - val_loss: 0.3910 - val_accuracy: 0.8455\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x794648de7070>"},"metadata":{}}]},{"cell_type":"markdown","source":"we load the ResNet50 model with pre-trained weights from the 'imagenet' dataset. We set include_top=False to exclude the top classification layers of ResNet50. The ResNet50 model is then added as the first layer of the CNN model.\n\nThe CNN layers are defined as before, with a Flatten layer followed by two dense layers for classification.\n\nFinally, the model is compiled and trained using the training_set and test_set data.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\n\n# Load ResNet50 model\nresnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\nresnet_model.trainable = False\n\n# Create CNN model\ncnn = tf.keras.models.Sequential()\ncnn.add(resnet_model)\ncnn.add(tf.keras.layers.Flatten())\ncnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\ncnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n\n# Compile the model\ncnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\ncnn.fit(x=training_set, validation_data=test_set, epochs=50)","metadata":{"execution":{"iopub.status.busy":"2023-09-05T22:41:48.331156Z","iopub.execute_input":"2023-09-05T22:41:48.331516Z","iopub.status.idle":"2023-09-05T22:55:03.314560Z","shell.execute_reply.started":"2023-09-05T22:41:48.331485Z","shell.execute_reply":"2023-09-05T22:55:03.313685Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/50\n264/264 [==============================] - 44s 125ms/step - loss: 0.6980 - accuracy: 0.5544 - val_loss: 0.6652 - val_accuracy: 0.7182\nEpoch 2/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.6586 - accuracy: 0.6030 - val_loss: 0.6453 - val_accuracy: 0.6258\nEpoch 3/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.6373 - accuracy: 0.6435 - val_loss: 0.5941 - val_accuracy: 0.6742\nEpoch 4/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.6039 - accuracy: 0.6811 - val_loss: 0.5610 - val_accuracy: 0.7530\nEpoch 5/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5815 - accuracy: 0.6978 - val_loss: 0.5510 - val_accuracy: 0.6773\nEpoch 6/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5712 - accuracy: 0.7065 - val_loss: 0.5258 - val_accuracy: 0.7379\nEpoch 7/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.5652 - accuracy: 0.7072 - val_loss: 0.5225 - val_accuracy: 0.7561\nEpoch 8/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5359 - accuracy: 0.7380 - val_loss: 0.5975 - val_accuracy: 0.6833\nEpoch 9/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5494 - accuracy: 0.7156 - val_loss: 0.5338 - val_accuracy: 0.7515\nEpoch 10/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5451 - accuracy: 0.7247 - val_loss: 0.5213 - val_accuracy: 0.7576\nEpoch 11/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.5317 - accuracy: 0.7368 - val_loss: 0.5006 - val_accuracy: 0.7576\nEpoch 12/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5092 - accuracy: 0.7512 - val_loss: 0.5228 - val_accuracy: 0.7167\nEpoch 13/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5133 - accuracy: 0.7516 - val_loss: 0.5056 - val_accuracy: 0.7288\nEpoch 14/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5102 - accuracy: 0.7474 - val_loss: 0.4997 - val_accuracy: 0.7606\nEpoch 15/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.5025 - accuracy: 0.7391 - val_loss: 0.5277 - val_accuracy: 0.7364\nEpoch 16/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5185 - accuracy: 0.7410 - val_loss: 0.4923 - val_accuracy: 0.7621\nEpoch 17/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.5143 - accuracy: 0.7448 - val_loss: 0.5045 - val_accuracy: 0.7303\nEpoch 18/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5037 - accuracy: 0.7493 - val_loss: 0.4870 - val_accuracy: 0.7652\nEpoch 19/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4989 - accuracy: 0.7592 - val_loss: 0.4983 - val_accuracy: 0.7394\nEpoch 20/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.5112 - accuracy: 0.7448 - val_loss: 0.7966 - val_accuracy: 0.5636\nEpoch 21/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.5177 - accuracy: 0.7410 - val_loss: 0.4875 - val_accuracy: 0.7682\nEpoch 22/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.5024 - accuracy: 0.7535 - val_loss: 0.4897 - val_accuracy: 0.7424\nEpoch 23/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.4990 - accuracy: 0.7600 - val_loss: 0.4813 - val_accuracy: 0.7561\nEpoch 24/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4920 - accuracy: 0.7577 - val_loss: 0.4768 - val_accuracy: 0.7758\nEpoch 25/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.4829 - accuracy: 0.7660 - val_loss: 0.5108 - val_accuracy: 0.7333\nEpoch 26/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.5058 - accuracy: 0.7539 - val_loss: 0.4791 - val_accuracy: 0.7682\nEpoch 27/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4958 - accuracy: 0.7622 - val_loss: 0.4897 - val_accuracy: 0.7652\nEpoch 28/50\n264/264 [==============================] - 13s 47ms/step - loss: 0.4974 - accuracy: 0.7546 - val_loss: 0.4818 - val_accuracy: 0.7697\nEpoch 29/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4771 - accuracy: 0.7637 - val_loss: 0.4927 - val_accuracy: 0.7727\nEpoch 30/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.4767 - accuracy: 0.7717 - val_loss: 0.4835 - val_accuracy: 0.7712\nEpoch 31/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4875 - accuracy: 0.7706 - val_loss: 0.5174 - val_accuracy: 0.7227\nEpoch 32/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4906 - accuracy: 0.7577 - val_loss: 0.5555 - val_accuracy: 0.6848\nEpoch 33/50\n264/264 [==============================] - 13s 47ms/step - loss: 0.4754 - accuracy: 0.7774 - val_loss: 0.4702 - val_accuracy: 0.7773\nEpoch 34/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4812 - accuracy: 0.7664 - val_loss: 0.4931 - val_accuracy: 0.7561\nEpoch 35/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.4764 - accuracy: 0.7637 - val_loss: 0.4785 - val_accuracy: 0.7742\nEpoch 36/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4805 - accuracy: 0.7740 - val_loss: 0.5480 - val_accuracy: 0.7076\nEpoch 37/50\n264/264 [==============================] - 13s 47ms/step - loss: 0.4754 - accuracy: 0.7656 - val_loss: 0.4823 - val_accuracy: 0.7621\nEpoch 38/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4789 - accuracy: 0.7649 - val_loss: 0.4919 - val_accuracy: 0.7424\nEpoch 39/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.4682 - accuracy: 0.7774 - val_loss: 0.4714 - val_accuracy: 0.7712\nEpoch 40/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4629 - accuracy: 0.7789 - val_loss: 0.4720 - val_accuracy: 0.7667\nEpoch 41/50\n264/264 [==============================] - 12s 47ms/step - loss: 0.4702 - accuracy: 0.7774 - val_loss: 0.4616 - val_accuracy: 0.7864\nEpoch 42/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4709 - accuracy: 0.7694 - val_loss: 0.4885 - val_accuracy: 0.7652\nEpoch 43/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.4796 - accuracy: 0.7736 - val_loss: 0.5112 - val_accuracy: 0.7409\nEpoch 44/50\n264/264 [==============================] - 11s 43ms/step - loss: 0.4939 - accuracy: 0.7531 - val_loss: 0.4717 - val_accuracy: 0.7833\nEpoch 45/50\n264/264 [==============================] - 13s 48ms/step - loss: 0.4657 - accuracy: 0.7823 - val_loss: 0.4620 - val_accuracy: 0.7818\nEpoch 46/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4690 - accuracy: 0.7740 - val_loss: 0.4851 - val_accuracy: 0.7652\nEpoch 47/50\n264/264 [==============================] - 12s 44ms/step - loss: 0.4821 - accuracy: 0.7759 - val_loss: 0.4939 - val_accuracy: 0.7606\nEpoch 48/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.4633 - accuracy: 0.7759 - val_loss: 0.4984 - val_accuracy: 0.7500\nEpoch 49/50\n264/264 [==============================] - 12s 45ms/step - loss: 0.4790 - accuracy: 0.7702 - val_loss: 0.4722 - val_accuracy: 0.7621\nEpoch 50/50\n264/264 [==============================] - 12s 46ms/step - loss: 0.4684 - accuracy: 0.7770 - val_loss: 0.4732 - val_accuracy: 0.7727\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7a2d0c1eece0>"},"metadata":{}}]}]}